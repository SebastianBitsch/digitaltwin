{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import supervision as sv\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE_DIR = \"../data/processed/Wildtrack_dataset/Image_subsets\"\n",
    "\n",
    "# SOURCE_VIDEO_PATH = \"../data/raw/Camera.mp4\"\n",
    "SOURCE_VIDEO_PATH = os.path.join(BASE_IMAGE_DIR, \"C1\")\n",
    "\n",
    "PERSON_DETECTION_MODEL = YOLO(\"../models/yolo11m.pt\")\n",
    "PERSON_ID = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 14:09:34.390 Python[7434:10171944] ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to /var/folders/kg/87d0nmxn11xbzs39mwh7hjvr0000gn/T/org.python.python.savedState\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/401 /Users/sebastianbitsch/Documents/Programmeringsprojekter/digitaltwin/notebooks/../data/processed/Wildtrack_dataset/Image_subsets/C1/00000000.png: 384x640 12 persons, 1 truck, 1 backpack, 1 handbag, 698.4ms\n",
      "image 2/401 /Users/sebastianbitsch/Documents/Programmeringsprojekter/digitaltwin/notebooks/../data/processed/Wildtrack_dataset/Image_subsets/C1/00000005.png: 384x640 14 persons, 2 backpacks, 514.7ms\n",
      "image 3/401 /Users/sebastianbitsch/Documents/Programmeringsprojekter/digitaltwin/notebooks/../data/processed/Wildtrack_dataset/Image_subsets/C1/00000010.png: 384x640 14 persons, 1 backpack, 536.5ms\n",
      "image 4/401 /Users/sebastianbitsch/Documents/Programmeringsprojekter/digitaltwin/notebooks/../data/processed/Wildtrack_dataset/Image_subsets/C1/00000015.png: 384x640 13 persons, 1 truck, 1 backpack, 532.3ms\n",
      "image 5/401 /Users/sebastianbitsch/Documents/Programmeringsprojekter/digitaltwin/notebooks/../data/processed/Wildtrack_dataset/Image_subsets/C1/00000020.png: 384x640 14 persons, 2 backpacks, 530.4ms\n",
      "image 6/401 /Users/sebastianbitsch/Documents/Programmeringsprojekter/digitaltwin/notebooks/../data/processed/Wildtrack_dataset/Image_subsets/C1/00000025.png: 384x640 13 persons, 1 truck, 3 backpacks, 528.1ms\n",
      "image 7/401 /Users/sebastianbitsch/Documents/Programmeringsprojekter/digitaltwin/notebooks/../data/processed/Wildtrack_dataset/Image_subsets/C1/00000030.png: 384x640 17 persons, 1 truck, 2 backpacks, 526.3ms\n",
      "image 8/401 /Users/sebastianbitsch/Documents/Programmeringsprojekter/digitaltwin/notebooks/../data/processed/Wildtrack_dataset/Image_subsets/C1/00000035.png: 384x640 11 persons, 2 handbags, 511.6ms\n",
      "image 9/401 /Users/sebastianbitsch/Documents/Programmeringsprojekter/digitaltwin/notebooks/../data/processed/Wildtrack_dataset/Image_subsets/C1/00000040.png: 384x640 16 persons, 1 backpack, 1 handbag, 523.2ms\n",
      "image 10/401 /Users/sebastianbitsch/Documents/Programmeringsprojekter/digitaltwin/notebooks/../data/processed/Wildtrack_dataset/Image_subsets/C1/00000045.png: 384x640 17 persons, 1 backpack, 2 handbags, 1 book, 527.2ms\n",
      "image 11/401 /Users/sebastianbitsch/Documents/Programmeringsprojekter/digitaltwin/notebooks/../data/processed/Wildtrack_dataset/Image_subsets/C1/00000050.png: 384x640 16 persons, 1 car, 1 backpack, 2 handbags, 512.3ms\n",
      "image 12/401 /Users/sebastianbitsch/Documents/Programmeringsprojekter/digitaltwin/notebooks/../data/processed/Wildtrack_dataset/Image_subsets/C1/00000055.png: 384x640 13 persons, 1 handbag, 539.1ms\n",
      "image 13/401 /Users/sebastianbitsch/Documents/Programmeringsprojekter/digitaltwin/notebooks/../data/processed/Wildtrack_dataset/Image_subsets/C1/00000060.png: 384x640 14 persons, 1 truck, 1 handbag, 527.5ms\n",
      "image 14/401 /Users/sebastianbitsch/Documents/Programmeringsprojekter/digitaltwin/notebooks/../data/processed/Wildtrack_dataset/Image_subsets/C1/00000065.png: 384x640 15 persons, 1 handbag, 528.1ms\n",
      "image 15/401 /Users/sebastianbitsch/Documents/Programmeringsprojekter/digitaltwin/notebooks/../data/processed/Wildtrack_dataset/Image_subsets/C1/00000070.png: 384x640 13 persons, 1 handbag, 542.2ms\n",
      "image 16/401 /Users/sebastianbitsch/Documents/Programmeringsprojekter/digitaltwin/notebooks/../data/processed/Wildtrack_dataset/Image_subsets/C1/00000075.png: 384x640 13 persons, 1 backpack, 2 handbags, 550.5ms\n",
      "image 17/401 /Users/sebastianbitsch/Documents/Programmeringsprojekter/digitaltwin/notebooks/../data/processed/Wildtrack_dataset/Image_subsets/C1/00000080.png: 384x640 14 persons, 2 handbags, 523.0ms\n"
     ]
    }
   ],
   "source": [
    "results = PERSON_DETECTION_MODEL.track(\n",
    "    source = SOURCE_VIDEO_PATH, \n",
    "    show=True, \n",
    "    conf=0.3, \n",
    "    tracker=\"botsort.yaml\", \n",
    "    # persist=True, \n",
    "    # stream=True\n",
    ")\n",
    "\n",
    "# for r in results:\n",
    "#     print(r)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "# frame = next(frame_generator)\n",
    "\n",
    "# sv.plot_image(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# box_annotator = sv.BoxAnnotator(\n",
    "#     color=sv.ColorPalette.from_hex(['#FF8C00', '#00BFFF', '#FF1493', '#FFD700']),\n",
    "#     thickness=2\n",
    "# )\n",
    "# label_annotator = sv.LabelAnnotator(\n",
    "#     color=sv.ColorPalette.from_hex(['#FF8C00', '#00BFFF', '#FF1493', '#FFD700']),\n",
    "#     text_color=sv.Color.from_hex('#000000')\n",
    "# )\n",
    "\n",
    "# frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "# frame = next(frame_generator)\n",
    "\n",
    "# result = PERSON_DETECTION_MODEL(frame, task=\"detect\", mode=\"predict\", conf=0.3)[0]\n",
    "# detections = sv.Detections.from_ultralytics(result)\n",
    "\n",
    "# labels = [\n",
    "#     f\"{class_name} {confidence:.2f}\"\n",
    "#     for class_name, confidence\n",
    "#     in zip(detections['class_name'], detections.confidence)\n",
    "# ]\n",
    "\n",
    "# annotated_frame = frame.copy()\n",
    "# annotated_frame = box_annotator.annotate(\n",
    "#     scene=annotated_frame,\n",
    "#     detections=detections)\n",
    "# annotated_frame = label_annotator.annotate(\n",
    "#     scene=annotated_frame,\n",
    "#     detections=detections,\n",
    "#     labels=labels)\n",
    "\n",
    "# sv.plot_image(annotated_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
    "\n",
    "# label_annotator = sv.LabelAnnotator(\n",
    "#     color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),\n",
    "#     text_color=sv.Color.from_hex('#000000'),\n",
    "#     text_position=sv.Position.BOTTOM_CENTER\n",
    "# )\n",
    "\n",
    "# tracker = sv.ByteTrack()\n",
    "# tracker.reset()\n",
    "\n",
    "# frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "# # frame = next(frame_generator)\n",
    "\n",
    "# for frame in tqdm(frame_generator, total=video_info.total_frames):\n",
    "#     result = PERSON_DETECTION_MODEL(frame, task=\"detect\", mode=\"predict\", conf=0.3)[0]\n",
    "#     detections = sv.Detections.from_ultralytics(result)\n",
    "\n",
    "#     all_detections = detections[detections.class_id == PERSON_ID]\n",
    "#     all_detections = all_detections.with_nms(threshold=0.5, class_agnostic=True)\n",
    "#     # all_detections.class_id -= 1\n",
    "#     all_detections = tracker.update_with_detections(detections=all_detections)\n",
    "\n",
    "#     labels = [\n",
    "#         f\"#{tracker_id}\"\n",
    "#         for tracker_id\n",
    "#         in all_detections.tracker_id\n",
    "#     ]\n",
    "\n",
    "#     annotated_frame = frame.copy()\n",
    "#     annotated_frame = label_annotator.annotate(\n",
    "#         scene=annotated_frame,\n",
    "#         detections=all_detections,\n",
    "#         labels=labels\n",
    "#     )\n",
    "\n",
    "#     sv.plot_image(annotated_frame)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
